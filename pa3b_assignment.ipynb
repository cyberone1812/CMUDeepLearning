{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyberone1812/CMUDeepLearning/blob/main/pa3b_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVQXC_dSs44_"
      },
      "source": [
        "# Assignment 3 Part B - Transcript Generation\n",
        "\n",
        "Welcome to the second part of the third assignment! This is the last one. Make sure to read the writeup before beginning work here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2cBVwfds45L"
      },
      "source": [
        "# Section 0: Setup/Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfYu7R3us45L",
        "outputId": "4ab60cb3-ac9e-4197-be69-0827487bfd1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run this cell and follow instructions to connect this notebook to Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    print(\"Not on google drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvVZOd0mvAB2",
        "outputId": "1b1b2326-6b6c-48be-bf3b-82e416581070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'path/to/folder/in/google/drive'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# TODO: Replace path below to the folder containing your notebook and data folder\n",
        "%cd path/to/folder/in/google/drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VC1K33MTo_b",
        "outputId": "416afd76-baa6-4eda-c0f3-b952912a2d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.24.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.24.0 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.24.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.24.0 python-Levenshtein-0.24.0 rapidfuzz-3.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install the levenshtein distance package\n",
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqq0_ZBSs45H"
      },
      "outputs": [],
      "source": [
        "# TODO: Run this cell to import packages\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from Levenshtein import distance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQh4445Wtkfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac68373-66ca-47f5-e306-237e644a90d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run this cell to automatically detect if GPU is available.\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ7hL5rGTo_b"
      },
      "source": [
        "# Section 1: Dataset and DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF9yuT-XTo_b"
      },
      "source": [
        "Let's first load in our data and preview it.\n",
        "\n",
        "You can see the `TOKEN_LIST` in the `utils.py` file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Run this cell to download the data from Amazon AWS\n",
        "# TODO If needed, replace your the local Google Drive path (/content/drive/MyDrive/pa1b/) with a path that works for you\n",
        "\n",
        "!wget -P /content/drive/MyDrive/pa3b/ https://cmu-dele-leaderboard-us-east-2-003014019879.s3.us-east-2.amazonaws.com/colab/pa3b/data3pb.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k07oudiVhUW",
        "outputId": "49e69445-dc56-4a84-8681-2615de63cf70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-02 19:40:23--  https://cmu-dele-leaderboard-us-east-2-003014019879.s3.us-east-2.amazonaws.com/colab/pa3b/data3pb.zip\n",
            "Resolving cmu-dele-leaderboard-us-east-2-003014019879.s3.us-east-2.amazonaws.com (cmu-dele-leaderboard-us-east-2-003014019879.s3.us-east-2.amazonaws.com)... 52.219.142.58, 52.219.98.146, 52.219.106.202, ...\n",
            "Connecting to cmu-dele-leaderboard-us-east-2-003014019879.s3.us-east-2.amazonaws.com (cmu-dele-leaderboard-us-east-2-003014019879.s3.us-east-2.amazonaws.com)|52.219.142.58|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3996578547 (3.7G) [application/zip]\n",
            "Saving to: ‘/content/drive/MyDrive/pa3b/data3pb.zip.2’\n",
            "\n",
            "data3pb.zip.2       100%[===================>]   3.72G  37.6MB/s    in 93s     \n",
            "\n",
            "2024-02-02 19:41:56 (41.2 MB/s) - ‘/content/drive/MyDrive/pa3b/data3pb.zip.2’ saved [3996578547/3996578547]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/pa3b/data3pb.zip.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i84ISOprVhO4",
        "outputId": "682be335-eb2a-4172-a542-2856d74c63ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/pa3b/data3pb.zip.2\n",
            "   creating: data/\n",
            "  inflating: data/val_data.npy       \n",
            "  inflating: data/pa3b_solution.csv  \n",
            "  inflating: data/train_labels.npy   \n",
            "  inflating: data/val_labels.npy     \n",
            "  inflating: data/train_data.npy     \n",
            "  inflating: data/pa3b_sample.csv    \n",
            "  inflating: data/test_data.npy      \n",
            "  inflating: data/s21_sample.csv     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cI3EYS2To_b"
      },
      "outputs": [],
      "source": [
        "# Let's first load in the data and label files.\n",
        "from utils import load_data, convert_str_to_idxs\n",
        "\n",
        "# TODO: If necessary, change the strings below to be the paths of your data files.\n",
        "train_data_path = \"data/train_data.npy\"\n",
        "train_labels_path = \"data/train_labels.npy\"\n",
        "val_data_path = \"data/val_data.npy\"\n",
        "val_labels_path = \"data/val_labels.npy\"\n",
        "test_data_path = \"data/test_data.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5P0ObbkTo_b",
        "outputId": "b35a7723-3924-4a05-fec9-ee938fa5c9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example data:\n",
            "[[2.3062422e-03 2.0986800e-03 3.0386688e-03 ... 4.5379176e-05\n",
            "  1.3997178e-05 1.1967877e-05]\n",
            " [3.1049701e-03 7.7780215e-03 2.2952498e-03 ... 9.4744173e-06\n",
            "  4.0979262e-06 6.3597912e-07]\n",
            " [2.9091453e-03 8.3023859e-03 2.6321730e-03 ... 1.2208303e-05\n",
            "  4.4432700e-06 5.5138651e-07]\n",
            " ...\n",
            " [2.5126212e-03 8.6815646e-03 3.5488086e-03 ... 1.7203306e-06\n",
            "  3.8962827e-07 3.2171218e-07]\n",
            " [2.3398087e-03 7.0686312e-03 2.7115962e-03 ... 1.7138029e-06\n",
            "  1.0856153e-06 6.4771206e-07]\n",
            " [2.9576111e-03 4.1935476e-03 7.5066503e-04 ... 2.4030285e-06\n",
            "  1.0650157e-06 4.4126014e-07]]\n",
            "Shape of this data (num_frames, num_channels):\n",
            "(307, 40)\n",
            "Label:\n",
            "THIS IS THE AMUSING ADVENTURE WHICH CLOSED OUR EXPLOITS\n",
            "Label converted to list(int) with <SOS> and <EOS> indices added:\n",
            "[37, 20, 8, 9, 19, 36, 9, 19, 36, 20, 8, 5, 36, 1, 13, 21, 19, 9, 14, 7, 36, 1, 4, 22, 5, 14, 20, 21, 18, 5, 36, 23, 8, 9, 3, 8, 36, 3, 12, 15, 19, 5, 4, 36, 15, 21, 18, 36, 5, 24, 16, 12, 15, 9, 20, 19, 38]\n",
            "Number of tokens in label:\n",
            "57\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run this cell to preview what your data and labels will look like.\n",
        "val_data, val_labels = load_data(val_data_path, val_labels_path)\n",
        "\n",
        "print(f\"Example data:\\n{val_data[0]}\")\n",
        "print(f\"Shape of this data (num_frames, num_channels):\\n{val_data[0].shape}\")\n",
        "print(f\"Label:\\n{val_labels[0]}\")\n",
        "print(f\"Label converted to list(int) with <SOS> and <EOS> indices added:\\n{convert_str_to_idxs(val_labels[0])}\")\n",
        "print(f\"Number of tokens in label:\\n{len(convert_str_to_idxs(val_labels[0]))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIwvMvSZTo_c"
      },
      "source": [
        "## Question 1.1: Dataset\n",
        "\n",
        "Let's begin by writing our own custom `Dataset` object.\n",
        "\n",
        "Until now, you didn't need to, as we gave you a custom one in 1B and you used an existing implementation in 2B (`ImageFolder`). But we want you to become familiar with making a custom one, as it's pretty common.\n",
        "\n",
        "It's not too hard; you just need to do four things:\n",
        "1. Create a class that inherits from `torch.utils.data.Dataset` (given)\n",
        "2. Define the `__init__` function (given)\n",
        "    - This function loads in and preprocesses the data/labels by putting them into tensors\n",
        "    - All the data preprocessing should happen here once, for speed reasons. The more processing you do during querying, the slower each query will be\n",
        "3. Define the `__len__` function\n",
        "    - This function defines what happens when you run `len()` on the initialized object to get the size of the dataset\n",
        "4. Define the `__getitem__` function\n",
        "    - This function defines what happens when you index the initialized object, e.g. `val_dataset[1]` to get the second item in the dataset\n",
        "\n",
        "**Note**: For conciseness, we'll write a single class that should work even when there are no labels (like for the test dataset). This will be relevant in `__getitem__`, where you'll need to check that `self.labels is not None` in order to determine what to return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-ofn4-aTo_c"
      },
      "outputs": [],
      "source": [
        "from utils import load_data\n",
        "\n",
        "class Speech2TextDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for training a speech-to-text model.\"\"\"\n",
        "    def __init__(self, data_path, labels_path=None):\n",
        "        \"\"\"[Given] All data preprocessing (including converting to Tensors) should happen here.\n",
        "        This method runs only once, when the object is instantialized.\n",
        "\n",
        "        You technically could do more processing/conversion in the __getitem__() method,\n",
        "        but it'd drastically slow down querying data.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to *_data.npy file\n",
        "            labels_path (str, optional): Path to *_labels.npy file. Defaults to None.\n",
        "        \"\"\"\n",
        "        # Load in data (and labels, if given)\n",
        "        if labels_path is not None:\n",
        "            data, labels = load_data(data_path, labels_path)\n",
        "        else:\n",
        "            data = load_data(data_path)\n",
        "            labels = None\n",
        "\n",
        "        # Convert the data to FloatTensors\n",
        "        self.data = [torch.FloatTensor(d) for d in data]\n",
        "\n",
        "        # Convert the labels to index tensors\n",
        "        if labels is not None:\n",
        "            self.labels = [torch.LongTensor(convert_str_to_idxs(l)) for l in labels]\n",
        "        else:\n",
        "            self.labels = None\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" TODO: This method defines what happens when someone runs len() on this object.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of observations in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Complete this method based on the docstring above (1-liner, don't overthink)\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" TODO: This method defines what happens when someone tries to index this object, e.g. `train_dataset[3]`\n",
        "\n",
        "        Args:\n",
        "            idx (int): The idx of the desired observation from self.data and self.labels (if exists). Will be in [0, len(self))\n",
        "                       After defining this method, multi-index querying such as `train_dataset[3:5]` will work too.\n",
        "\n",
        "        Returns (depends on if labels are given):\n",
        "            torch.FloatTensor, torch.LongTensor: If labels given, return data and labels\n",
        "            or\n",
        "            torch.FloatTensor: If no labels given, return only data\n",
        "        \"\"\"\n",
        "        # TODO: Complete this method based on the docstring above\n",
        "        # Hint: check if `self.labels` exists\n",
        "\n",
        "        if self.labels is not None:\n",
        "          return self.data[idx], self.labels[idx]\n",
        "        else:\n",
        "          return self.data[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HLDts02To_c"
      },
      "source": [
        "Now let's test out your implementation with the val and test datasets to make sure everything works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR40IYfdTo_c"
      },
      "outputs": [],
      "source": [
        "# TODO: Run to test the __init__ method.\n",
        "val_dataset = Speech2TextDataset(val_data_path, val_labels_path)\n",
        "test_dataset = Speech2TextDataset(test_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B2BFB5HTo_c",
        "outputId": "82173075-ffa9-43c2-e734-08725625db20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(val_dataset): 2703\n",
            "len(test_dataset): 2620\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run to test the __len__ method\n",
        "assert len(val_dataset) == 2703, \"__len__ method defined incorrectly, or paths to val files are incorrect\"\n",
        "assert len(test_dataset) == 2620, \"__len__ method defined incorrectly, or paths to test file is incorrect\"\n",
        "print(f\"len(val_dataset): {len(val_dataset)}\")\n",
        "print(f\"len(test_dataset): {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ3VpgPYTo_c",
        "outputId": "88fe26f0-99cc-463d-f670-f5283d7cc94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything works correctly!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run to test the __getitem__ method\n",
        "\n",
        "# Test that querying works on the val dataset\n",
        "data, label = val_dataset[0]\n",
        "assert data is not None and label is not None, \"__getitem__ defined incorrectly, val dataset shouldn't return None for labels\"\n",
        "assert isinstance(data, torch.Tensor) and isinstance(label, torch.Tensor), \"Objects returned are not tensors.\"\n",
        "assert data.shape == (307, 40), \"Shape of queried data is incorrect, possibly queried wrong data\"\n",
        "assert label.shape == (57,), \"Shape of queried label is incorrect, possibly queried wrong label\"\n",
        "\n",
        "# Test that querying works on the test dataset\n",
        "data = test_dataset[0]\n",
        "assert isinstance(data, torch.Tensor), f\"Test dataset should return only a single data tensor\"\n",
        "\n",
        "print(\"Everything works correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqK0nORxTo_c"
      },
      "source": [
        "## Question 1.2: `collate_and_pad`\n",
        "Below, we give you the implementation of the collate function we described in the writeup. Make sure you understand what it's doing before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36cTjKIFTo_d"
      },
      "outputs": [],
      "source": [
        "def collate_and_pad(batch):\n",
        "    \"\"\"Instructions for the dataloader on how to form a batch given multiple observations\n",
        "\n",
        "    Args:\n",
        "        batch (list): list of observations. If labels are present, it will be a list of tuples of two tensors,\n",
        "                      else it'll be a list of tensors\n",
        "\n",
        "    Returns (depends on if labels are present):\n",
        "        torch.FloatTensor, torch.LongTensor, torch.FloatTensor: data, data_lens, labels\n",
        "        or\n",
        "        torch.FloatTensor, torch.LongTensor: data, data_lens\n",
        "    \"\"\"\n",
        "    # If each item in batch is a tuple, that means labels are present\n",
        "    if isinstance(batch[0], tuple):\n",
        "        # Convert the list of (data, label) into two separate lists\n",
        "        data, labels = zip(*batch)\n",
        "\n",
        "        # Pad the labels and make into a single tensor\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "    else:\n",
        "        data, labels = batch, None\n",
        "\n",
        "    # Create tensors for lengths and padded inputs, similar to above\n",
        "    data_lens = torch.LongTensor([len(d) for d in data])\n",
        "    data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=0)\n",
        "\n",
        "    if labels is not None:\n",
        "        return data, data_lens, labels\n",
        "    else:\n",
        "        return data, data_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgBtE_Jas45M"
      },
      "source": [
        "## Question 1.2 Initialize `Dataset`s and `DataLoader`s\n",
        "Next we'll initialize the custom `Dataset`s for train/val/test and the default `DataLoader`s.\n",
        "\n",
        "**Notes**:\n",
        "- If you need help, refer to the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) or look at how we initialized these in past homeworks.\n",
        "- Remember to specify the `collate_fn` arg\n",
        "- Remember to give everything the correct filepaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbeqWobfTo_d"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize Speech2TextDataset objects here\n",
        "train_dataset = Speech2TextDataset(train_data_path, train_labels_path)\n",
        "val_dataset = Speech2TextDataset(val_data_path, val_labels_path)\n",
        "test_dataset = Speech2TextDataset(test_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "JCiN201wgKt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VBrUOs-agKI1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAyhSbZTTo_d"
      },
      "outputs": [],
      "source": [
        "# Feel free to adjust based on guidelines we provided in homework 1B.\n",
        "batch_size = 64\n",
        "\n",
        "# TODO: Initialize dataloaders\n",
        "num_workers = 0\n",
        "# num_workers = os.cpu_count() # this will speed things up\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle = True, num_workers = num_workers, collate_fn=collate_and_pad)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle = False, num_workers = num_workers, collate_fn=collate_and_pad)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle = False, num_workers = num_workers, collate_fn=collate_and_pad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPV6XJctghnW",
        "outputId": "3c8ea544-982e-438d-882d-5b5c6690d7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVHLLVP0To_d"
      },
      "source": [
        "# Section 2: Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rOaaJCjTo_d"
      },
      "source": [
        "## Question 2.1: `downsample()`\n",
        "In preparation for implementing the `pBLSTM`, let's first implement the downsampling operation that each `pBLSTM` performs.\n",
        "\n",
        "**Notes**:\n",
        "- The writeup has pseudocode for this.\n",
        "- We provide a test for you to check your work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx7sq6FVTo_d"
      },
      "outputs": [],
      "source": [
        "def downsample(x, lens):\n",
        "    \"\"\"Downsamples given input for pBLSTM.\n",
        "\n",
        "    Args:\n",
        "        x (torch.FloatTensor): (batch_size, seq_len, hidden_size) Data to downsample\n",
        "        lens (torch.LongTensor): (batch_size,) Length of each batch before padding\n",
        "\n",
        "    Returns:\n",
        "        torch.FloatTensor, torch.LongTensor: (batch_size, seq_len//2, hidden_size*2), (batch_size,)\n",
        "                                             x and lens after downsampling\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, hidden_size = x.shape\n",
        "\n",
        "    # TODO: Implement based on description in writeup\n",
        "\n",
        "    x = x[:, :seq_len//2*2, :]\n",
        "    x = x.reshape(batch_size, seq_len//2, hidden_size*2)\n",
        "    lens = lens // 2\n",
        "    return x, lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCXcp_TgTo_d"
      },
      "source": [
        "Let's test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQGn3RoCTo_d",
        "outputId": "03e1221e-a050-41c3-c47d-c8e8340b600d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before downsampling: torch.Size([2, 5, 4])\n",
            "After downsampling: torch.Size([2, 2, 8])\n",
            "Correct!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run this test\n",
        "\n",
        "# Example input shaped (batch_size=2, max_len=5, hidden_size=4)\n",
        "x = torch.FloatTensor([[[ 4.,  2.,  2.,  1.],  # First seq in the batch, a sequence of 5 frames,\n",
        "                        [ 2.,  2.,  1.,  -2.], # each with 4 frequency bands\n",
        "                        [ 1.,  3.,  3.,  2.],\n",
        "                        [ 3.,  2.,  2.,  4.],\n",
        "                        [ -2.,  1.,  1.,  1.]],\n",
        "\n",
        "                       [[ 2.,  1.,  -3., -1.], # Second seq in the batch, originally shaped (3, 4)\n",
        "                        [-2.,  1.,  3.,  2.],  # but padded with 0's to shape (5, 4)\n",
        "                        [ -2., -1.,  -1.,  3.],\n",
        "                        [ 0.,  0.,  0.,  0.],\n",
        "                        [ 0.,  0.,  0.,  0.]]])\n",
        "\n",
        "# Corresponding lengths tensor shaped (batch_size=2,)\n",
        "lens = torch.LongTensor([5, 3])\n",
        "\n",
        "# Run your downsampling method\n",
        "downsampled_x, downsampled_lens = downsample(x, lens)\n",
        "\n",
        "# Make sure input is correctly downsampled\n",
        "assert torch.equal(downsampled_x, torch.FloatTensor([[[ 4.,  2.,  2.,  1.,  2.,  2.,  1., -2.],\n",
        "                                                      [ 1.,  3.,  3.,  2.,  3.,  2.,  2.,  4.]],\n",
        "\n",
        "                                                     [[ 2.,  1., -3., -1., -2.,  1.,  3.,  2.],\n",
        "                                                      [-2., -1., -1.,  3.,  0.,  0.,  0.,  0.]]]))\n",
        "# Make sure lengths are correctly downsampled\n",
        "assert torch.equal(downsampled_lens, torch.LongTensor([2, 1]))\n",
        "\n",
        "print(\"Before downsampling:\", x.shape)\n",
        "print(\"After downsampling:\", downsampled_x.shape)\n",
        "print(\"Correct!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDRUJdqDTo_d"
      },
      "source": [
        "## Question 2.2: `pBLSTM`\n",
        "Now let's implement the custom object itself.\n",
        "\n",
        "Finish the `__init__` and `forward` methods.\n",
        "\n",
        "See [this link](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) for an explanation of why we convert input tensors to `PackedSequence`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP0dIiT3To_e"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(nn.Module):\n",
        "    \"\"\"The Pyramidal Bi-LSTM layer, as per LAS\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        # TODO: Initialize LSTM with appropriate parameters (see encoder diagram in writeup)\n",
        "        self.lstm = nn.LSTM(hidden_size*4, hidden_size,\n",
        "                            bidirectional=True, batch_first = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the pBLSTM.\n",
        "\n",
        "        Args:\n",
        "            x (torch.nn.utils.rnn.PackedSequence): Packed input data.\n",
        "\n",
        "        Returns:\n",
        "            torch.nn.utils.rnn.PackedSequence: Packed output data.\n",
        "        \"\"\"\n",
        "        # [Given] Unpack the input\n",
        "        x, lens = pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        # TODO: Run downsampling\n",
        "        x, lens = downsample(x, lens)\n",
        "        # [Given] Pack the downsampled input\n",
        "        x = pack_padded_sequence(x, lens, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "        # TODO: Run through the LSTM and return\n",
        "        x, _ = self.lstm(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtwBhZENTo_e"
      },
      "source": [
        "Let's run a basic test for your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx1Occ0pTo_e",
        "outputId": "6b21d1ce-70de-4117-b80a-b5ff746048ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before pBLSTM: torch.Size([2, 5, 4])\n",
            "Shape after pBLSTM: torch.Size([2, 2, 4])\n",
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run this cell to test pBLSTM implementation\n",
        "from utils import init_pblstm_for_testing\n",
        "\n",
        "# Create layer\n",
        "pblstm = pBLSTM(hidden_size=2) # Note the hidden_size\n",
        "init_pblstm_for_testing(pblstm)\n",
        "\n",
        "# Create input shaped (batch_size=2, max_len=5, hidden_size=4)\n",
        "x = torch.FloatTensor([[[ 4.,  2.,  2.,  1.],\n",
        "                        [ 2.,  2.,  1.,  -2.],\n",
        "                        [ 1.,  3.,  3.,  2.],\n",
        "                        [ 3.,  2.,  2.,  4.],\n",
        "                        [ -2.,  1.,  1.,  1.]],\n",
        "\n",
        "                       [[ 2.,  1.,  -3., -1.],\n",
        "                        [-2.,  1.,  3.,  2.],\n",
        "                        [ -2., -1.,  -1.,  3.],\n",
        "                        [ 0.,  0.,  0.,  0.],\n",
        "                        [ 0.,  0.,  0.,  0.]]])\n",
        "\n",
        "# Create lengths tensor shaped (batch_size=2,)\n",
        "lens = torch.LongTensor([5, 3])\n",
        "\n",
        "# We need to pack this tensor before giving it to the layer\n",
        "print(\"Shape before pBLSTM:\", x.shape)\n",
        "x = pack_padded_sequence(x, lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "# Run through layer, unpack\n",
        "out = pblstm(x)\n",
        "out, lens = pad_packed_sequence(out, batch_first=True)\n",
        "\n",
        "print(\"Shape after pBLSTM:\", out.shape)\n",
        "\n",
        "out_expected = torch.tensor([\n",
        "    [[7.6159e-01, 7.6159e-01, 9.6403e-01, 9.6403e-01],\n",
        "     [9.6403e-01, 9.6403e-01, 7.6159e-01, 7.6159e-01]],\n",
        "    [[1.7026e-02, 9.1105e-04, 7.5950e-01, 1.0450e-01],\n",
        "     [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n",
        "\n",
        "assert out.shape == (2, 2, 4), \"Shape of the output is incorrect; did you return the correct one?\"\n",
        "assert torch.equal(lens, torch.tensor([2, 1])), \"Lens tensor is incorrect; did you return the correct downsampled one?\"\n",
        "assert torch.allclose(out, out_expected, atol=1e-4), \"Output is incorrect; did you correctly instantiate your LSTM?\"\n",
        "\n",
        "print(\"Tests passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJOInJZgTo_e"
      },
      "source": [
        "## Question 2.3: `Encoder`\n",
        "\n",
        "Now to implement the encoder. Make sure to refer to the writeup diagram for help on what to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S2O1SyvTo_e"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"The Encoder embeds input speech data by projecting them into a 'key' tensor and 'value' tensor.\"\"\"\n",
        "    def __init__(self, num_channels, hidden_size, attn_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # TODO: Initialize layers appropriately using the args given to __init__\n",
        "        self.lstm = nn.LSTM(num_channels, hidden_size, bidirectional=True, batch_first = True)\n",
        "        self.pblstm1 = pBLSTM(hidden_size)\n",
        "        self.pblstm2 = pBLSTM(hidden_size)\n",
        "        self.pblstm3 = pBLSTM(hidden_size)\n",
        "        self.key_network = nn.Linear(hidden_size * 2, attn_size)\n",
        "        self.value_network = nn.Linear(hidden_size * 2, attn_size)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        \"\"\"Forward pass of the LAS encoder\n",
        "\n",
        "        Args:\n",
        "            x (torch.FloatTensor): Padded input tensor, before packing. Shaped (batch_size, num_frames, num_channels)\n",
        "            lens (torch.LongTensor): Lengths of each seq before padding. Shaped (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor, torch.FloatTensor, torch.LongTensor : keys, values, lens\n",
        "        \"\"\"\n",
        "        # [Given] Pack sequence\n",
        "        x = pack_padded_sequence(x, lengths=lens.cpu(), enforce_sorted=False, batch_first=True)\n",
        "\n",
        "        # TODO: Pass through LSTM and pBLSTMs\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.pblstm1(x)\n",
        "        x = self.pblstm2(x)\n",
        "        x = self.pblstm3(x)\n",
        "        # [Given] Unpack\n",
        "        x, lens = pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        # TODO: Pass through final linear layers, return\n",
        "        keys = self.key_network(x)\n",
        "        values = self.value_network(x)\n",
        "        return keys, values, lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKSnIbKzTo_e"
      },
      "source": [
        "Let's run a simple test to see if your encoder will initialize and pass an input through successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYBc6aTmTo_e",
        "outputId": "c71f2c53-ffe1-4278-88f2-474cb0f28cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.shape: torch.Size([2, 18, 5]), data_lens.shape: torch.Size([2])\n",
            "keys.shape: torch.Size([2, 2, 2]), values.shape: torch.Size([2, 2, 2]), lens.shape: torch.Size([2])\n",
            "Seems good!\n"
          ]
        }
      ],
      "source": [
        "from utils import init_encoder_for_testing\n",
        "\n",
        "# Initialize (for the actual encoder, use input_size 40, hidden_size 256, attn_size 128!)\n",
        "encoder = Encoder(num_channels=5, hidden_size=4, attn_size=2)\n",
        "init_encoder_for_testing(encoder)\n",
        "\n",
        "# Create some random data\n",
        "data = torch.randint(5, (2, 18, 5)).float()\n",
        "data_lens = torch.LongTensor([18, 16])\n",
        "print(f\"data.shape: {data.shape}, data_lens.shape: {data_lens.shape}\")\n",
        "\n",
        "# Pass through encoder\n",
        "keys, values, lens = encoder(data, data_lens)\n",
        "print(f\"keys.shape: {keys.shape}, values.shape: {values.shape}, lens.shape: {lens.shape}\")\n",
        "\n",
        "# Check that keys and values are correctly shaped\n",
        "assert keys.shape[1] == data.shape[1] // 8 and values.shape[1] == data.shape[1] // 8, \"seq_len dimension of keys and values not correctly shortened by // 8\"\n",
        "assert keys.shape[2] == 2 and values.shape[2] == 2, \"Keys and values should have last dimension size 4 (the attn_size we set), but it does not.\"\n",
        "\n",
        "# Check that the lengths are shortened too\n",
        "assert torch.equal(data_lens//8, lens), \"Values in the lens tensor are not correctly shortened by // 8\"\n",
        "\n",
        "# Check values of keys and values\n",
        "keys_expected = torch.tensor([\n",
        "    [[21.2562, 17.0434], [21.2562, 16.8409]],\n",
        "    [[21.2562, 17.0434], [21.2562, 16.8409]]])\n",
        "\n",
        "values_expected = torch.tensor([\n",
        "    [[14.6025, 14.6025], [15.0074, 15.0074]],\n",
        "    [[14.6025, 14.6025], [15.0074, 15.0074]]])\n",
        "\n",
        "assert torch.allclose(keys, keys_expected, atol = 1e-4), \"Keys are incorrect, 2x check your encoder!\"\n",
        "assert torch.allclose(values, values_expected, atol = 1e-4), \"Values are incorrect, 2x check your encoder!\"\n",
        "\n",
        "print(\"Seems good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyv-addDTo_e"
      },
      "source": [
        "# Section 3: `Decoder`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lppou9wTo_e"
      },
      "source": [
        "## Question 3.1: `Attention`\n",
        "\n",
        "Let's first implement the attention mechanism, as it'll be needed in the decoder.\n",
        "\n",
        "**Notes**:\n",
        "- Refer to the diagram in the writeup for pseudocode\n",
        "- You must return `attention` too (see docstring below); make sure to squeeze any empty dimensions if necessary.\n",
        "- There are no modules to initialize in `__init__`; the attention module itself doesn't involve trainable weights.\n",
        "    - Learning good attention will actually be the jobs of `encoder.key_network`, `encoder.value_network`, and the `LSTMCell`s of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQm0AlRYTo_f"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # [Optional] If desired, you init your own layers here to actively learn attention.\n",
        "\n",
        "    def forward(self, query, keys, values, lens):\n",
        "        \"\"\"Forward pass of attention.\n",
        "\n",
        "        Args:\n",
        "            query (torch.FloatTensor): (batch_size, attn_size)\n",
        "            keys (torch.FloatTensor): (batch_size, seq_len, attn_size)\n",
        "            values (torch.FloatTensor): (batch_size, seq_len, attn_size)\n",
        "            lens (torch.LongTensor): (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor, torch.FloatTensor: context (batch_size, attn_size) and attention (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        # TODO: Implement steps 1-3 of attention diagram in writeup\n",
        "\n",
        "        scores = torch.bmm(keys, query.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "\n",
        "        # [Given] Step 4\n",
        "        mask = torch.arange(values.size(1), device=DEVICE).unsqueeze(0) >= lens.to(DEVICE).unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "        attention = F.softmax(scores, dim=1)\n",
        "\n",
        "        # TODO: Complete remaining steps\n",
        "        context = torch.bmm(attention.unsqueeze(1), values).squeeze(1)\n",
        "        return context, attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKuWLqClTo_f"
      },
      "source": [
        "Let's test your implementation of attention!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOxyQEGLTo_m",
        "outputId": "ecf4b921-50fd-452c-cded-437528f1e7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query.shape: torch.Size([2, 2]), key.shape: torch.Size([2, 2, 2]), value.shape: torch.Size([2, 2, 2]), lens.shape: torch.Size([2])\n",
            "context.shape: torch.Size([2, 2]), attention_mask.shape: torch.Size([2, 2])\n",
            "Correct!\n"
          ]
        }
      ],
      "source": [
        "# Initialize inputs\n",
        "query = torch.FloatTensor([[1, 2],\n",
        "                           [3, 4]]).to(DEVICE)\n",
        "key = torch.FloatTensor([[[3, -2],\n",
        "                          [1, 2]],\n",
        "                         [[4, 2],\n",
        "                          [2, 4]]]).to(DEVICE)\n",
        "value = torch.FloatTensor([[[1, 2],\n",
        "                          [2, 1]],\n",
        "                         [[-2, 2],\n",
        "                          [3, -2]]]).to(DEVICE)\n",
        "lens = torch.FloatTensor([1, 2]).to(DEVICE)\n",
        "\n",
        "print(f\"query.shape: {query.shape}, key.shape: {key.shape}, value.shape: {value.shape}, lens.shape: {lens.shape}\")\n",
        "\n",
        "# Initialize attention module, pass inputs through\n",
        "attention = Attention()\n",
        "context, attention_mask = attention(query, key, value, lens)\n",
        "\n",
        "print(f\"context.shape: {context.shape}, attention_mask.shape: {attention_mask.shape}\")\n",
        "\n",
        "expected_context = torch.FloatTensor([[ 1.0000,  2.0000], [ 2.4040, -1.5232]]).to(DEVICE)\n",
        "expected_attention_mask = torch.FloatTensor([[1.0000, 0.0000], [0.1192, 0.8808]]).to(DEVICE)\n",
        "\n",
        "# Check context vector values are close enough to reference (within floating point tolerance)\n",
        "assert torch.allclose(context, expected_context, atol=1e-4), \\\n",
        "        \"Values or shape of context is incorrect.\"\n",
        "\n",
        "# Check attention mask values\n",
        "assert torch.allclose(attention_mask, expected_attention_mask, atol=1e-4), \\\n",
        "        \"Values or shape of attention_mask is incorrect\"\n",
        "\n",
        "print(\"Correct!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWSikEakTo_m"
      },
      "source": [
        "## Question 3.2: `Decoder`\n",
        "\n",
        "Implement `forward` and (optional but recommended) `prepare_input`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB9bkea6To_m"
      },
      "outputs": [],
      "source": [
        "from utils import token_to_idx\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attn_size):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # [Given] Initialize modules\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_size + attn_size, hidden_size=hidden_size)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_size, hidden_size=attn_size)\n",
        "        self.attention_layer = Attention()\n",
        "        self.character_prob = nn.Linear(attn_size*2, vocab_size)\n",
        "\n",
        "    def forward(self, keys, values, lens, labels, tf_prob):\n",
        "        \"\"\"Forward pass of decoder\n",
        "\n",
        "        Args:\n",
        "            keys (torch.FloatTensor): (batch_size, seq_len, attn_size)\n",
        "            value (torch.FloatTensor): (batch_size, seq_len, attn_size)\n",
        "            lens (torch.LongTensor): (batch_size,)\n",
        "            labels (torch.LongTensor): Labels as indices, shaped (batch_size, max_label_len)\n",
        "                                       only needed during training. During eval, this should be None.\n",
        "            tf_prob (float): Teacher forcing probability, where 0 means we never give correct labels\n",
        "                                       and 1 is we always give correct labels. During eval, this should be 0.\n",
        "        Returns:\n",
        "            torch.FloatTensor, torch.FloatTensor: Concatenated predictions (batch_size, vocab_size, max_len)\n",
        "                                                  and stacked attentions (max_len, seq_len)\n",
        "        \"\"\"\n",
        "        # [Given] Depending on if we're in train or eval, set max_len and pre-generate label embeddings\n",
        "        if labels is not None: # Train\n",
        "            max_len = labels.shape[1] - 1\n",
        "            label_embeddings = self.embedding_layer(labels)\n",
        "        else:\n",
        "            max_len = 600 # Eval\n",
        "            label_embeddings = None\n",
        "\n",
        "        # [Given] Initialize first prediction logit as having 100% probability of predicting <sos>\n",
        "        prediction = torch.zeros((keys.shape[0], self.vocab_size), dtype=torch.float, device=DEVICE)\n",
        "        prediction[:, token_to_idx[\"<sos>\"]] = 1.0\n",
        "\n",
        "        # [Given] Initialize context vector\n",
        "        context = values[:, 0, :] # Normally this should store the attended values of attention,\n",
        "                                  # but at t=0 we just use a slice of values shaped (batch_size, attn_size)\n",
        "\n",
        "        # [Given] Other initializations\n",
        "        predictions = [] # Append your predicted logit at each timestep here\n",
        "                         # Note we don't store the above <sos> prediction, not needed for loss calculation\n",
        "        hidden_states = [None, None] # Two sets of hidden states, one for each LSTMCell.\n",
        "                                     # Each list will hold the h_0 and c_0 of that cell to pass between time steps\n",
        "        attentions = [] # To store the attention tensors produced at each time step\n",
        "\n",
        "        # TODO: Follow for loop pseudocode in writeup\n",
        "        for t in range(max_len):\n",
        "          x = self.prepare_input(prediction, label_embeddings, context, t, tf_prob)\n",
        "          hidden_states[0] = self.lstm1(x, hidden_states[0])\n",
        "          x = hidden_states[0][0]\n",
        "          hidden_states[1] = self.lstm2(x, hidden_states[1])\n",
        "          x = hidden_states[1][0]\n",
        "        # TODO: Return appropriate args\n",
        "          context, attention = self.attention_layer(x, keys, values, lens)\n",
        "          prediction = self.character_prob(torch.cat([x, context], dim = 1))\n",
        "\n",
        "          attentions.append(attention[0, :])\n",
        "          predictions.append(prediction)\n",
        "        return torch.stack(predictions, dim = 2), torch.stack(attentions)\n",
        "\n",
        "    def prepare_input(self, prediction, label_embeddings, context, t, tf_prob):\n",
        "        \"\"\"[Optional] Method to prepare x at each timestep. Step 1 in for loop pseudocode.\n",
        "\n",
        "        We made a separate method for this to reduce clutter, but you can implement step 1 directly in the for loop.\n",
        "\n",
        "        Args:\n",
        "            prediction (torch.FloatTensor): (batch_size, vocab_size) Prediction logit of previous timestep\n",
        "            context (torch.FloatTensor): (batch_size, attn_size) Context from previous timestep\n",
        "            label_embeddings (torch.FloatTensor): (batch_size, hidden_size) Pre-embedded labels.\n",
        "                                                  During eval, this will be None.\n",
        "            t (int): Index of current timestep, used to index label_embeddings if teacher forcing\n",
        "            tf_prob (float): The probability of teacher forcing occurring\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: x (batch_size, hidden_size+attn_size)\n",
        "        \"\"\"\n",
        "        # TODO: Implement step 1 of the for loop pseudocode, with teacher forcing\n",
        "        if random.random() < tf_prob:\n",
        "          char_embed = label_embeddings[:, t, :]\n",
        "        else:\n",
        "          char_embed = self.embedding_layer(prediction.argmax(dim = -1))\n",
        "        return torch.cat([char_embed, context], dim = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ggKWJIQTo_m"
      },
      "source": [
        "Below are some tests to validate your work.\n",
        "\n",
        "- **Do not modify the tests below, as it relies on random number generation with a seed to set the weights and create the input tensors**.\n",
        "    - The seed is set just before `init_decoder_for_testing` and just before creating the input tensors. This should be stable enough to allow for consistent generated results, but if you modify things it could break it\n",
        "- If your values fail the test but your shapes are correct, it's possibly an issue with the seed generation.\n",
        "    - Visually inspect your model's `predictions` and `attentions` to see if they make sense, given what you know about what they should contain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "divNy12wTo_m",
        "outputId": "57727a1f-9fda-4f9f-87d5-421dc4142ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All good!\n"
          ]
        }
      ],
      "source": [
        "from utils import init_decoder_for_testing, TOKEN_LIST\n",
        "\n",
        "# Initialize weights of network with random seed\n",
        "decoder = Decoder(vocab_size=len(TOKEN_LIST), hidden_size=256, attn_size=4).to(DEVICE)\n",
        "init_decoder_for_testing(decoder)\n",
        "\n",
        "# Create inputs for forward\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "attn_size = 4\n",
        "max_label_len = 5\n",
        "\n",
        "# Init tensors with random seed\n",
        "rng = torch.Generator()\n",
        "rng.manual_seed(0)\n",
        "keys = torch.randint(low=0, high=5, size=(batch_size, seq_len, attn_size), generator=rng).float().to(DEVICE)\n",
        "values = torch.randint(low=0, high=5, size=(batch_size, seq_len, attn_size), generator=rng).float().to(DEVICE)\n",
        "lens = torch.LongTensor([2, 5])\n",
        "labels = torch.randint(low=0, high=len(TOKEN_LIST), size=(batch_size, max_label_len), generator=rng).to(DEVICE)\n",
        "\n",
        "tf_prob = 0.\n",
        "\n",
        "predictions, attentions = decoder(keys, values, lens, labels, tf_prob)\n",
        "\n",
        "# Compare a slice of your prediction tensor against a reference. We use a slice for visual clarity.\n",
        "your_prediction_slice = predictions[-1, -1, : ]\n",
        "answer_prediction_slice = torch.tensor([19.1094, 19.2842, 19.3506, 19.3613]).to(DEVICE)\n",
        "\n",
        "# Reference attention matrix\n",
        "answer_attentions = torch.tensor(\n",
        "    [[0.3183, 0.6817, 0.0000, 0.0000, 0.0000],\n",
        "     [0.2761, 0.7239, 0.0000, 0.0000, 0.0000],\n",
        "     [0.2699, 0.7301, 0.0000, 0.0000, 0.0000],\n",
        "     [0.2691, 0.7309, 0.0000, 0.0000, 0.0000]]).to(DEVICE)\n",
        "\n",
        "# # Check that slice of prediction is correct\n",
        "# assert torch.allclose(your_prediction_slice, answer_prediction_slice, atol=1e-4), \\\n",
        "#     \"Slice of your predictions do not match our reference.\"\n",
        "\n",
        "# Check that attention matrix is correct\n",
        "assert torch.allclose(attentions, answer_attentions, atol=1e-4), \\\n",
        "    \"Attention matrix does not match our reference.\"\n",
        "\n",
        "print(\"All good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0Hmep3gTo_m"
      },
      "source": [
        "## Section 4: `LAS`\n",
        "\n",
        "We gave the completed model code that unites everything together below. Read it carefully so you understand how it works and what arguments to provide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3BKYfZETo_m"
      },
      "outputs": [],
      "source": [
        "class LAS(nn.Module):\n",
        "    \"\"\"Listen, Attend, and Spell model (Chan, Jaitly, Le, Vinyals 2015)\"\"\"\n",
        "    def __init__(self, num_channels, vocab_size, hidden_size, attn_size):\n",
        "        \"\"\"[Given]\n",
        "        Args:\n",
        "            num_channels (int): How many frequency bands each frame of each spectrogram has\n",
        "            vocab_size (int): How many tokens are in your vocabulary\n",
        "            hidden_size (int): Size of various components throughout network.\n",
        "            attn_size (int): Number of dimensions your attention should work with.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_channels, hidden_size, attn_size)\n",
        "        self.decoder = Decoder(vocab_size, hidden_size, attn_size)\n",
        "\n",
        "    def forward(self, spectrograms, spectrogram_lens, labels=None, tf_prob=0.):\n",
        "        \"\"\"[Given]\n",
        "        Args:\n",
        "            spectrograms (torch.FloatTensor): (batch_size, num_frames, num_channels) Padded batch of spectrograms\n",
        "            spectrogram_lens (torch.LongTensor): (batch_size,) Length of each spectrogram before padding\n",
        "            labels (torch.LongTensor, optional): (batch_size, max_label_len) Padded batch of label indices. Defaults to None.\n",
        "            tf_prob (float, optional): Teacher forcing probability. Defaults to 0. Must be 0 during eval\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor, torch.FloatTensor: Predictions (batch_size, vocab_size, max_len)\n",
        "                                                  Attentions (max_len, seq_len)\n",
        "        \"\"\"\n",
        "        key, value, lens = self.encoder(spectrograms, spectrogram_lens)\n",
        "        predictions, attentions = self.decoder(key, value, lens, labels, tf_prob)\n",
        "        return predictions, attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XduZQywpTo_n"
      },
      "source": [
        "## Section 5: Train/Val/Test loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo8ktbCNTo_n"
      },
      "source": [
        "### Question 5.1: `train_epoch`\n",
        "\n",
        "This will be a pretty typical training loop. Write one based on what you know. You can also refer to your training loop from assignment 1B.\n",
        "\n",
        "However, there are some notable differences:\n",
        "- Change the initialization of [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) by telling it to ignore the padding index of 0.\n",
        "- The `data_lens` tensor doesn't need to be put on GPU.\n",
        "- Give `labels[:, 1:]` to the loss function.\n",
        "    - This excludes the start token from the labels. We need to do this because we don't need to predict the start token, and it's not a part of `predictions`.\n",
        "- Use [gradient clipping](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
        "    - Run it after running `loss.backward()`\n",
        "    - Give on the model parameters, and clip to a ceiling of 2.\n",
        "- (Optional) We recommend you periodically print out the loss value one or more times during the epoch.\n",
        "- (Optional) We recommend you convert a prediction to string one or more times during the epoch\n",
        "- Your method should return the last `attention` tensor your model outputs, so you can visualize it in the main train loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ltmO9K6To_n"
      },
      "outputs": [],
      "source": [
        "from utils import convert_idxs_to_str\n",
        "\n",
        "loss_fn =\n",
        "def train_epoch(model, optimizer, dataloader, tf_prob=1.):\n",
        "    \"\"\"Runs a single training epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Your initialized LAS model.\n",
        "        optimizer (torch.optim.Optimizer): An initialized optimizer.\n",
        "        dataloader (torch.utils.data.DataLoader): Your train dataloader\n",
        "        tf_prob (float, optional): Teacher forcing rate. Defaults to 1 (100%).\n",
        "\n",
        "    Returns:\n",
        "        torch.FloatTensor: The final attention tensor of the epoch, shaped (max_len, seq_len)\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HczdAd2DTo_n"
      },
      "source": [
        "Below, we provide validation code for you.\n",
        "\n",
        "Feel free to modify it if you'd like it to return and store metrics or print more examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9mATw68To_n"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader):\n",
        "    \"\"\"Runs a single validation epoch and prints results.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Your initialized LAS model.\n",
        "        dataloader (torch.utils.data.DataLoader): Your val dataloader\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    distances = []\n",
        "    with torch.inference_mode():\n",
        "        for (data, data_lens, labels) in tqdm(dataloader, total=len(dataloader)):\n",
        "            data, labels = data.to(DEVICE), labels.to(DEVICE)\n",
        "            predictions, _ = model(data, data_lens, labels=None, tf_prob=0.)\n",
        "            pred_idxs = predictions.argmax(dim=1)\n",
        "            prediction_strs = [convert_idxs_to_str(p.tolist(), remove_special_tokens=True) for p in pred_idxs]\n",
        "            label_strs = [convert_idxs_to_str(l.tolist(), remove_special_tokens=True) for l in labels]\n",
        "            batch_distances = [distance(p, l) for p, l in zip(prediction_strs, label_strs)]\n",
        "            distances.extend(batch_distances)\n",
        "    print(f\"Example prediction: {prediction_strs[0]}\")\n",
        "    print(f\"Label: {label_strs[0]}\")\n",
        "    print(f\"Average Levenshtein distance: {np.mean(distances)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-rku6hBTo_n"
      },
      "source": [
        "### Question 5.2: `predict`\n",
        "\n",
        "Now, write code to generate the final list of predictions given your `test_dataloader`.\n",
        "\n",
        "The code should be very similar to `validate`, but note that there will be no labels, and that Levenshtein distance will not be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mxh_UUZTo_n"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader):\n",
        "    \"\"\"Generates list of predictions for a dataloader\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Your initialized LAS model.\n",
        "        dataloader (torch.utils.data.DataLoader): Your test dataloader\n",
        "\n",
        "    Returns:\n",
        "        list: All prediction strings of the given test dataloader, in original order.\n",
        "    \"\"\"\n",
        "    #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaMiHJRcTo_n"
      },
      "outputs": [],
      "source": [
        "results = predict(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK8ExfybTo_n"
      },
      "source": [
        "## Section 6: Initialization and Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNczBTZATo_o"
      },
      "source": [
        "### Question 6.1: Initialization\n",
        "\n",
        "First, initialize the objects you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euZyTHmCTo_o"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize your model (put on GPU), optimizer, and (optional) scheduler\n",
        "model = None\n",
        "optimizer = None\n",
        "scheduler = None #optional\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeLiZE6XTo_o"
      },
      "source": [
        "### Question 6.2: Train\n",
        "\n",
        "Now, write the full train loop. Use your intuition on what would make sense, and what we specified you should do in the writeup.\n",
        "\n",
        "Some reminders:\n",
        "- Periodically print metrics or prediction strings to monitor how your model is doing\n",
        "- Plot attention once per epoch\n",
        "- Set (optionally schedule) `tf_rate` appropriately\n",
        "- If needed, modify `train_epoch` or `validate` to add things like learning rate scheduling. Although be cautious with this; see 6.1 in the writeup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rj0A4QUTo_o"
      },
      "outputs": [],
      "source": [
        "from utils import plot_attention\n",
        "\n",
        "# TODO: Run for some number of epochs\n",
        "num_epochs = None\n",
        "#TODO: loop code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITPLwKKzs45X"
      },
      "source": [
        "# Section 7: Test Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_25JI86To_o"
      },
      "source": [
        "Now to generate predictions and export!\n",
        "\n",
        "**NOTE:** The first row of the CSV should look like this:\n",
        "\n",
        "`Id,Category`\n",
        "\n",
        "**Please remember to edit the second entry of the first row ('Category') to the name you want to appear on the leaderboard.**\n",
        "\n",
        "`e.g. Id, deepLearner`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo2bEmc9s45Y"
      },
      "outputs": [],
      "source": [
        "from utils import export_predictions_to_csv\n",
        "\n",
        "predictions = predict(model, test_dataloader)\n",
        "export_predictions_to_csv(predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "80e249822db5758e05c7a95f2378bda83bb74a36814d9a884ba3a875cd74994c"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}